{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow MNIST\n",
    "\n",
    "Based on the TensorFlow tutorial at http://tensorflow.org/tutorials/mnist/pros/index.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import idx2numpy\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "figsize(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = \"/Users/jeffreyschecter/Desktop/mnist/\"\n",
    "train_im = idx2numpy.convert_from_file(prefix + \"train-images-idx3-ubyte\")\n",
    "test_im = idx2numpy.convert_from_file(prefix + \"t10k-images-idx3-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = idx2numpy.convert_from_file(prefix + \"train-labels-idx1-ubyte\")\n",
    "test_labels = idx2numpy.convert_from_file(prefix + \"t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throw a random forest at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97140000000000004"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1)\n",
    "clf.fit(train_im.reshape(60000, 28 * 28), train_labels)\n",
    "model_labels = clf.predict(test_im.reshape(10000, 28 * 28))\n",
    "(model_labels == test_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple softmax regression w/ TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no existing session to delete\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del(sess)\n",
    "    print \"deleted session\"\n",
    "except Exception as e:\n",
    "    print \"no existing session to delete\"\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28 * 28])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "W = tf.Variable(tf.zeros([28 * 28, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "xent = -tf.reduce_mean(y_ * tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(xent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels_one_hot = np.zeros((60000, 10))\n",
    "train_labels_one_hot[np.arange(60000), train_labels] = 1\n",
    "test_labels_one_hot = np.zeros((10000, 10))\n",
    "test_labels_one_hot[np.arange(10000), test_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "0.893\n",
      "Starting training epoch 1\n",
      "0.9041\n",
      "Starting training epoch 2\n",
      "0.9092\n",
      "Starting training epoch 3\n",
      "0.9115\n",
      "Starting training epoch 4\n",
      "0.9139\n",
      "Starting training epoch 5\n",
      "0.9156\n",
      "Starting training epoch 6\n",
      "0.9165\n",
      "Starting training epoch 7\n",
      "0.9177\n",
      "Starting training epoch 8\n",
      "0.9186\n",
      "Starting training epoch 9\n",
      "0.9195\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "epochs = 10\n",
    "for ep in xrange(epochs):\n",
    "    print \"Starting training epoch {ep}\".format(ep=ep)\n",
    "    for i in xrange(train_im.shape[0] / batch_size):\n",
    "        start_offset = i * batch_size\n",
    "        stop_offset = start_offset + batch_size\n",
    "        train_step.run(feed_dict={\n",
    "            x: train_im[start_offset:stop_offset].reshape(batch_size, 28 * 28),\n",
    "            y_: train_labels_one_hot[start_offset:stop_offset]\n",
    "        })\n",
    "    accuracy_expr = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)), tf.float32))\n",
    "    accuracy = accuracy_expr.eval(feed_dict={\n",
    "        x: test_im.reshape(10000, 28 * 28),\n",
    "        y_: test_labels_one_hot})\n",
    "    print accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ConvNet w/ TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted session\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del(sess)\n",
    "    print \"deleted session\"\n",
    "except Exception as e:\n",
    "    print \"no existing session to delete\"\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.random_uniform(shape, minval=-0.01, maxval=0.01)\n",
    "    return tf.Variable(initial, name=\"weights\")\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial, name=\"bias\")\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=\"conv\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(\n",
    "        x, ksize=[1, 2, 2, 1],\n",
    "        strides=[1, 2, 2, 1], padding='SAME', name=\"max_pool\")\n",
    "\n",
    "def elu(x):\n",
    "    pos = tf.cast(tf.greater_equal(x, 0), tf.float32)\n",
    "    return (pos * x) + ((1 - pos) * (tf.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Internal Structure and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare elu vs. tf.nnet.relu\n",
    "nonlin = elu\n",
    "\n",
    "with tf.name_scope(\"conv1\") as scope:\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = nonlin(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "with tf.name_scope(\"conv2\") as scope:\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = nonlin(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "with tf.name_scope(\"dense\") as scope:\n",
    "    W_dense = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_dense = bias_variable([1024])\n",
    "    h_dense = nonlin(tf.matmul(tf.reshape(h_pool2, [-1, 7 * 7 * 64]), W_dense) + b_dense)\n",
    "\n",
    "with tf.name_scope(\"dropout\") as scope:\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    h_dropout = tf.nn.dropout(h_dense, keep_prob)\n",
    "\n",
    "with tf.name_scope(\"softmax_output\") as scope:\n",
    "    W_out = weight_variable([1024, 10])\n",
    "    b_out = bias_variable([10])\n",
    "    y = tf.nn.softmax(tf.matmul(h_dropout, W_out) + b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, Evaluation, and Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xent = -tf.reduce_mean(y_ * tf.log(y))\n",
    "learn_rate = tf.placeholder(tf.float32, [])\n",
    "train_step = tf.train.AdamOptimizer(learn_rate).minimize(xent)\n",
    "correct = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0 / 300: learn rate = 0.0001\n",
      "\n",
      "Finished epoch 0, batch 0: accuracy = 0.37\n",
      "\n",
      "epoch 0, batch 1 / 300: learn rate = 9.94649571761e-05\n",
      "epoch 0, batch 2 / 300: learn rate = 9.89327770605e-05\n",
      "epoch 0, batch 3 / 300: learn rate = 9.84034443363e-05\n",
      "epoch 0, batch 4 / 300: learn rate = 9.7876943769e-05\n",
      "epoch 0, batch 5 / 300: learn rate = 9.73532602051e-05\n",
      "epoch 0, batch 6 / 300: learn rate = 9.68323785726e-05\n",
      "epoch 0, batch 7 / 300: learn rate = 9.63142838798e-05\n",
      "epoch 0, batch 8 / 300: learn rate = 9.57989612155e-05\n",
      "epoch 0, batch 9 / 300: learn rate = 9.52863957482e-05\n",
      "epoch 0, batch 10 / 300: learn rate = 9.47765727256e-05\n",
      "epoch 0, batch 11 / 300: learn rate = 9.42694774745e-05\n",
      "epoch 0, batch 12 / 300: learn rate = 9.37650954002e-05\n",
      "epoch 0, batch 13 / 300: learn rate = 9.3263411986e-05\n",
      "epoch 0, batch 14 / 300: learn rate = 9.27644127928e-05\n",
      "epoch 0, batch 15 / 300: learn rate = 9.22680834591e-05\n",
      "epoch 0, batch 16 / 300: learn rate = 9.17744096998e-05\n",
      "epoch 0, batch 17 / 300: learn rate = 9.12833773065e-05\n",
      "epoch 0, batch 18 / 300: learn rate = 9.07949721468e-05\n",
      "epoch 0, batch 19 / 300: learn rate = 9.03091801639e-05\n",
      "epoch 0, batch 20 / 300: learn rate = 8.98259873762e-05\n",
      "\n",
      "Finished epoch 0, batch 20: accuracy = 0.76\n",
      "\n",
      "epoch 0, batch 21 / 300: learn rate = 8.93453798767e-05\n",
      "epoch 0, batch 22 / 300: learn rate = 8.88673438332e-05\n",
      "epoch 0, batch 23 / 300: learn rate = 8.83918654873e-05\n",
      "epoch 0, batch 24 / 300: learn rate = 8.79189311541e-05\n",
      "epoch 0, batch 25 / 300: learn rate = 8.74485272221e-05\n",
      "epoch 0, batch 26 / 300: learn rate = 8.69806401526e-05\n",
      "epoch 0, batch 27 / 300: learn rate = 8.65152564793e-05\n",
      "epoch 0, batch 28 / 300: learn rate = 8.6052362808e-05\n",
      "epoch 0, batch 29 / 300: learn rate = 8.5591945816e-05\n",
      "epoch 0, batch 30 / 300: learn rate = 8.51339922521e-05\n",
      "epoch 0, batch 31 / 300: learn rate = 8.46784889359e-05\n",
      "epoch 0, batch 32 / 300: learn rate = 8.42254227574e-05\n",
      "epoch 0, batch 33 / 300: learn rate = 8.37747806771e-05\n",
      "epoch 0, batch 34 / 300: learn rate = 8.33265497248e-05\n",
      "epoch 0, batch 35 / 300: learn rate = 8.28807170002e-05\n",
      "epoch 0, batch 36 / 300: learn rate = 8.24372696715e-05\n",
      "epoch 0, batch 37 / 300: learn rate = 8.19961949759e-05\n",
      "epoch 0, batch 38 / 300: learn rate = 8.15574802188e-05\n",
      "epoch 0, batch 39 / 300: learn rate = 8.11211127736e-05\n",
      "epoch 0, batch 40 / 300: learn rate = 8.0687080081e-05\n",
      "\n",
      "Finished epoch 0, batch 40: accuracy = 0.88\n",
      "\n",
      "epoch 0, batch 41 / 300: learn rate = 8.02553696492e-05\n",
      "epoch 0, batch 42 / 300: learn rate = 7.98259690532e-05\n",
      "epoch 0, batch 43 / 300: learn rate = 7.93988659341e-05\n",
      "epoch 0, batch 44 / 300: learn rate = 7.89740479997e-05\n",
      "epoch 0, batch 45 / 300: learn rate = 7.85515030232e-05\n",
      "epoch 0, batch 46 / 300: learn rate = 7.81312188432e-05\n",
      "epoch 0, batch 47 / 300: learn rate = 7.77131833636e-05\n",
      "epoch 0, batch 48 / 300: learn rate = 7.72973845528e-05\n",
      "epoch 0, batch 49 / 300: learn rate = 7.68838104437e-05\n",
      "epoch 0, batch 50 / 300: learn rate = 7.64724491332e-05\n",
      "epoch 0, batch 51 / 300: learn rate = 7.60632887818e-05\n",
      "epoch 0, batch 52 / 300: learn rate = 7.56563176136e-05\n",
      "epoch 0, batch 53 / 300: learn rate = 7.52515239154e-05\n",
      "epoch 0, batch 54 / 300: learn rate = 7.48488960368e-05\n",
      "epoch 0, batch 55 / 300: learn rate = 7.44484223898e-05\n",
      "epoch 0, batch 56 / 300: learn rate = 7.40500914483e-05\n",
      "epoch 0, batch 57 / 300: learn rate = 7.3653891748e-05\n",
      "epoch 0, batch 58 / 300: learn rate = 7.32598118857e-05\n",
      "epoch 0, batch 59 / 300: learn rate = 7.28678405194e-05\n",
      "epoch 0, batch 60 / 300: learn rate = 7.24779663678e-05\n",
      "\n",
      "Finished epoch 0, batch 60: accuracy = 0.93\n",
      "\n",
      "epoch 0, batch 61 / 300: learn rate = 7.20901782098e-05\n",
      "epoch 0, batch 62 / 300: learn rate = 7.17044648846e-05\n",
      "epoch 0, batch 63 / 300: learn rate = 7.13208152908e-05\n",
      "epoch 0, batch 64 / 300: learn rate = 7.09392183867e-05\n",
      "epoch 0, batch 65 / 300: learn rate = 7.05596631894e-05\n",
      "epoch 0, batch 66 / 300: learn rate = 7.01821387749e-05\n",
      "epoch 0, batch 67 / 300: learn rate = 6.98066342778e-05\n",
      "epoch 0, batch 68 / 300: learn rate = 6.94331388905e-05\n",
      "epoch 0, batch 69 / 300: learn rate = 6.90616418634e-05\n",
      "epoch 0, batch 70 / 300: learn rate = 6.86921325046e-05\n",
      "epoch 0, batch 71 / 300: learn rate = 6.83246001791e-05\n",
      "epoch 0, batch 72 / 300: learn rate = 6.79590343089e-05\n",
      "epoch 0, batch 73 / 300: learn rate = 6.75954243726e-05\n",
      "epoch 0, batch 74 / 300: learn rate = 6.72337599052e-05\n",
      "epoch 0, batch 75 / 300: learn rate = 6.68740304976e-05\n",
      "epoch 0, batch 76 / 300: learn rate = 6.65162257964e-05\n",
      "epoch 0, batch 77 / 300: learn rate = 6.61603355036e-05\n",
      "epoch 0, batch 78 / 300: learn rate = 6.58063493762e-05\n",
      "epoch 0, batch 79 / 300: learn rate = 6.54542572262e-05\n",
      "epoch 0, batch 80 / 300: learn rate = 6.510404892e-05\n",
      "\n",
      "Finished epoch 0, batch 80: accuracy = 0.94\n",
      "\n",
      "epoch 0, batch 81 / 300: learn rate = 6.47557143782e-05\n",
      "epoch 0, batch 82 / 300: learn rate = 6.44092435754e-05\n",
      "epoch 0, batch 83 / 300: learn rate = 6.40646265397e-05\n",
      "epoch 0, batch 84 / 300: learn rate = 6.37218533528e-05\n",
      "epoch 0, batch 85 / 300: learn rate = 6.33809141492e-05\n",
      "epoch 0, batch 86 / 300: learn rate = 6.30417991163e-05\n",
      "epoch 0, batch 87 / 300: learn rate = 6.27044984941e-05\n",
      "epoch 0, batch 88 / 300: learn rate = 6.23690025746e-05\n",
      "epoch 0, batch 89 / 300: learn rate = 6.2035301702e-05\n",
      "epoch 0, batch 90 / 300: learn rate = 6.1703386272e-05\n",
      "epoch 0, batch 91 / 300: learn rate = 6.13732467317e-05\n",
      "epoch 0, batch 92 / 300: learn rate = 6.10448735792e-05\n",
      "epoch 0, batch 93 / 300: learn rate = 6.07182573638e-05\n",
      "epoch 0, batch 94 / 300: learn rate = 6.0393388685e-05\n",
      "epoch 0, batch 95 / 300: learn rate = 6.00702581927e-05\n",
      "epoch 0, batch 96 / 300: learn rate = 5.9748856587e-05\n",
      "epoch 0, batch 97 / 300: learn rate = 5.94291746175e-05\n",
      "epoch 0, batch 98 / 300: learn rate = 5.91112030834e-05\n",
      "epoch 0, batch 99 / 300: learn rate = 5.87949328332e-05\n",
      "epoch 0, batch 100 / 300: learn rate = 5.84803547643e-05\n",
      "\n",
      "Finished epoch 0, batch 100: accuracy = 0.96\n",
      "\n",
      "epoch 0, batch 101 / 300: learn rate = 5.81674598227e-05\n",
      "epoch 0, batch 102 / 300: learn rate = 5.78562390031e-05\n",
      "epoch 0, batch 103 / 300: learn rate = 5.75466833481e-05\n",
      "epoch 0, batch 104 / 300: learn rate = 5.72387839485e-05\n",
      "epoch 0, batch 105 / 300: learn rate = 5.69325319425e-05\n",
      "epoch 0, batch 106 / 300: learn rate = 5.66279185159e-05\n",
      "epoch 0, batch 107 / 300: learn rate = 5.63249349016e-05\n",
      "epoch 0, batch 108 / 300: learn rate = 5.60235723793e-05\n",
      "epoch 0, batch 109 / 300: learn rate = 5.57238222756e-05\n",
      "epoch 0, batch 110 / 300: learn rate = 5.54256759634e-05\n",
      "epoch 0, batch 111 / 300: learn rate = 5.51291248615e-05\n",
      "epoch 0, batch 112 / 300: learn rate = 5.48341604351e-05\n",
      "epoch 0, batch 113 / 300: learn rate = 5.45407741946e-05\n",
      "epoch 0, batch 114 / 300: learn rate = 5.42489576962e-05\n",
      "epoch 0, batch 115 / 300: learn rate = 5.3958702541e-05\n",
      "epoch 0, batch 116 / 300: learn rate = 5.36700003752e-05\n",
      "epoch 0, batch 117 / 300: learn rate = 5.33828428896e-05\n",
      "epoch 0, batch 118 / 300: learn rate = 5.30972218196e-05\n",
      "epoch 0, batch 119 / 300: learn rate = 5.28131289446e-05\n",
      "epoch 0, batch 120 / 300: learn rate = 5.25305560881e-05\n",
      "\n",
      "Finished epoch 0, batch 120: accuracy = 0.95\n",
      "\n",
      "epoch 0, batch 121 / 300: learn rate = 5.22494951174e-05\n",
      "epoch 0, batch 122 / 300: learn rate = 5.19699379432e-05\n",
      "epoch 0, batch 123 / 300: learn rate = 5.16918765197e-05\n",
      "epoch 0, batch 124 / 300: learn rate = 5.14153028439e-05\n",
      "epoch 0, batch 125 / 300: learn rate = 5.11402089556e-05\n",
      "epoch 0, batch 126 / 300: learn rate = 5.08665869375e-05\n",
      "epoch 0, batch 127 / 300: learn rate = 5.05944289143e-05\n",
      "epoch 0, batch 128 / 300: learn rate = 5.03237270531e-05\n",
      "epoch 0, batch 129 / 300: learn rate = 5.00544735628e-05\n",
      "epoch 0, batch 130 / 300: learn rate = 4.9786660694e-05\n",
      "epoch 0, batch 131 / 300: learn rate = 4.95202807387e-05\n",
      "epoch 0, batch 132 / 300: learn rate = 4.92553260302e-05\n",
      "epoch 0, batch 133 / 300: learn rate = 4.89917889429e-05\n",
      "epoch 0, batch 134 / 300: learn rate = 4.87296618919e-05\n",
      "epoch 0, batch 135 / 300: learn rate = 4.84689373329e-05\n",
      "epoch 0, batch 136 / 300: learn rate = 4.82096077618e-05\n",
      "epoch 0, batch 137 / 300: learn rate = 4.79516657151e-05\n",
      "epoch 0, batch 138 / 300: learn rate = 4.76951037688e-05\n",
      "epoch 0, batch 139 / 300: learn rate = 4.74399145387e-05\n",
      "epoch 0, batch 140 / 300: learn rate = 4.71860906803e-05\n",
      "\n",
      "Finished epoch 0, batch 140: accuracy = 0.96\n",
      "\n",
      "epoch 0, batch 141 / 300: learn rate = 4.69336248882e-05\n",
      "epoch 0, batch 142 / 300: learn rate = 4.66825098963e-05\n",
      "epoch 0, batch 143 / 300: learn rate = 4.64327384771e-05\n",
      "epoch 0, batch 144 / 300: learn rate = 4.61843034419e-05\n",
      "epoch 0, batch 145 / 300: learn rate = 4.59371976406e-05\n",
      "epoch 0, batch 146 / 300: learn rate = 4.56914139611e-05\n",
      "epoch 0, batch 147 / 300: learn rate = 4.54469453296e-05\n",
      "epoch 0, batch 148 / 300: learn rate = 4.520378471e-05\n",
      "epoch 0, batch 149 / 300: learn rate = 4.49619251037e-05\n",
      "epoch 0, batch 150 / 300: learn rate = 4.472135955e-05\n",
      "epoch 0, batch 151 / 300: learn rate = 4.4482081125e-05\n",
      "epoch 0, batch 152 / 300: learn rate = 4.4244082942e-05\n",
      "epoch 0, batch 153 / 300: learn rate = 4.40073581512e-05\n",
      "epoch 0, batch 154 / 300: learn rate = 4.37718999395e-05\n",
      "epoch 0, batch 155 / 300: learn rate = 4.353770153e-05\n",
      "epoch 0, batch 156 / 300: learn rate = 4.33047561822e-05\n",
      "epoch 0, batch 157 / 300: learn rate = 4.30730571919e-05\n",
      "epoch 0, batch 158 / 300: learn rate = 4.28425978904e-05\n",
      "epoch 0, batch 159 / 300: learn rate = 4.26133716448e-05\n",
      "epoch 0, batch 160 / 300: learn rate = 4.23853718578e-05\n",
      "\n",
      "Finished epoch 0, batch 160: accuracy = 0.98\n",
      "\n",
      "epoch 0, batch 161 / 300: learn rate = 4.21585919673e-05\n",
      "epoch 0, batch 162 / 300: learn rate = 4.19330254463e-05\n",
      "epoch 0, batch 163 / 300: learn rate = 4.17086658028e-05\n",
      "epoch 0, batch 164 / 300: learn rate = 4.14855065795e-05\n",
      "epoch 0, batch 165 / 300: learn rate = 4.12635413536e-05\n",
      "epoch 0, batch 166 / 300: learn rate = 4.10427637367e-05\n",
      "epoch 0, batch 167 / 300: learn rate = 4.08231673746e-05\n",
      "epoch 0, batch 168 / 300: learn rate = 4.06047459471e-05\n",
      "epoch 0, batch 169 / 300: learn rate = 4.03874931678e-05\n",
      "epoch 0, batch 170 / 300: learn rate = 4.01714027838e-05\n",
      "epoch 0, batch 171 / 300: learn rate = 3.9956468576e-05\n",
      "epoch 0, batch 172 / 300: learn rate = 3.97426843582e-05\n",
      "epoch 0, batch 173 / 300: learn rate = 3.95300439775e-05\n",
      "epoch 0, batch 174 / 300: learn rate = 3.93185413139e-05\n",
      "epoch 0, batch 175 / 300: learn rate = 3.91081702802e-05\n",
      "epoch 0, batch 176 / 300: learn rate = 3.88989248215e-05\n",
      "epoch 0, batch 177 / 300: learn rate = 3.86907989157e-05\n",
      "epoch 0, batch 178 / 300: learn rate = 3.84837865726e-05\n",
      "epoch 0, batch 179 / 300: learn rate = 3.82778818342e-05\n",
      "epoch 0, batch 180 / 300: learn rate = 3.80730787743e-05\n",
      "\n",
      "Finished epoch 0, batch 180: accuracy = 0.97\n",
      "\n",
      "epoch 0, batch 181 / 300: learn rate = 3.78693714985e-05\n",
      "epoch 0, batch 182 / 300: learn rate = 3.76667541439e-05\n",
      "epoch 0, batch 183 / 300: learn rate = 3.74652208788e-05\n",
      "epoch 0, batch 184 / 300: learn rate = 3.72647659031e-05\n",
      "epoch 0, batch 185 / 300: learn rate = 3.70653834473e-05\n",
      "epoch 0, batch 186 / 300: learn rate = 3.6867067773e-05\n",
      "epoch 0, batch 187 / 300: learn rate = 3.66698131725e-05\n",
      "epoch 0, batch 188 / 300: learn rate = 3.64736139686e-05\n",
      "epoch 0, batch 189 / 300: learn rate = 3.62784645144e-05\n",
      "epoch 0, batch 190 / 300: learn rate = 3.60843591934e-05\n",
      "epoch 0, batch 191 / 300: learn rate = 3.5891292419e-05\n",
      "epoch 0, batch 192 / 300: learn rate = 3.56992586345e-05\n",
      "epoch 0, batch 193 / 300: learn rate = 3.5508252313e-05\n",
      "epoch 0, batch 194 / 300: learn rate = 3.53182679571e-05\n",
      "epoch 0, batch 195 / 300: learn rate = 3.51293000989e-05\n",
      "epoch 0, batch 196 / 300: learn rate = 3.49413432997e-05\n",
      "epoch 0, batch 197 / 300: learn rate = 3.47543921498e-05\n",
      "epoch 0, batch 198 / 300: learn rate = 3.45684412686e-05\n",
      "epoch 0, batch 199 / 300: learn rate = 3.43834853043e-05\n",
      "epoch 0, batch 200 / 300: learn rate = 3.41995189335e-05\n",
      "\n",
      "Finished epoch 0, batch 200: accuracy = 0.98\n",
      "\n",
      "epoch 0, batch 201 / 300: learn rate = 3.40165368617e-05\n",
      "epoch 0, batch 202 / 300: learn rate = 3.38345338223e-05\n",
      "epoch 0, batch 203 / 300: learn rate = 3.36535045771e-05\n",
      "epoch 0, batch 204 / 300: learn rate = 3.34734439158e-05\n",
      "epoch 0, batch 205 / 300: learn rate = 3.32943466563e-05\n",
      "epoch 0, batch 206 / 300: learn rate = 3.31162076437e-05\n",
      "epoch 0, batch 207 / 300: learn rate = 3.29390217512e-05\n",
      "epoch 0, batch 208 / 300: learn rate = 3.2762783879e-05\n",
      "epoch 0, batch 209 / 300: learn rate = 3.2587488955e-05\n",
      "epoch 0, batch 210 / 300: learn rate = 3.24131319339e-05\n",
      "epoch 0, batch 211 / 300: learn rate = 3.22397077974e-05\n",
      "epoch 0, batch 212 / 300: learn rate = 3.20672115544e-05\n",
      "epoch 0, batch 213 / 300: learn rate = 3.18956382402e-05\n",
      "epoch 0, batch 214 / 300: learn rate = 3.17249829167e-05\n",
      "epoch 0, batch 215 / 300: learn rate = 3.15552406722e-05\n",
      "epoch 0, batch 216 / 300: learn rate = 3.13864066214e-05\n",
      "epoch 0, batch 217 / 300: learn rate = 3.12184759051e-05\n",
      "epoch 0, batch 218 / 300: learn rate = 3.10514436901e-05\n",
      "epoch 0, batch 219 / 300: learn rate = 3.08853051689e-05\n",
      "epoch 0, batch 220 / 300: learn rate = 3.07200555599e-05\n",
      "\n",
      "Finished epoch 0, batch 220: accuracy = 0.98\n",
      "\n",
      "epoch 0, batch 221 / 300: learn rate = 3.05556901072e-05\n",
      "epoch 0, batch 222 / 300: learn rate = 3.039220408e-05\n",
      "epoch 0, batch 223 / 300: learn rate = 3.0229592773e-05\n",
      "epoch 0, batch 224 / 300: learn rate = 3.00678515062e-05\n",
      "epoch 0, batch 225 / 300: learn rate = 2.99069756244e-05\n",
      "epoch 0, batch 226 / 300: learn rate = 2.97469604975e-05\n",
      "epoch 0, batch 227 / 300: learn rate = 2.958780152e-05\n",
      "epoch 0, batch 228 / 300: learn rate = 2.94294941113e-05\n",
      "epoch 0, batch 229 / 300: learn rate = 2.92720337149e-05\n",
      "epoch 0, batch 230 / 300: learn rate = 2.91154157991e-05\n",
      "epoch 0, batch 231 / 300: learn rate = 2.89596358562e-05\n",
      "epoch 0, batch 232 / 300: learn rate = 2.88046894028e-05\n",
      "epoch 0, batch 233 / 300: learn rate = 2.86505719792e-05\n",
      "epoch 0, batch 234 / 300: learn rate = 2.84972791498e-05\n",
      "epoch 0, batch 235 / 300: learn rate = 2.83448065027e-05\n",
      "epoch 0, batch 236 / 300: learn rate = 2.81931496496e-05\n",
      "epoch 0, batch 237 / 300: learn rate = 2.80423042256e-05\n",
      "epoch 0, batch 238 / 300: learn rate = 2.78922658891e-05\n",
      "epoch 0, batch 239 / 300: learn rate = 2.77430303221e-05\n",
      "epoch 0, batch 240 / 300: learn rate = 2.75945932292e-05\n",
      "\n",
      "Finished epoch 0, batch 240: accuracy = 0.98\n",
      "\n",
      "epoch 0, batch 241 / 300: learn rate = 2.74469503384e-05\n",
      "epoch 0, batch 242 / 300: learn rate = 2.73000974002e-05\n",
      "epoch 0, batch 243 / 300: learn rate = 2.71540301882e-05\n",
      "epoch 0, batch 244 / 300: learn rate = 2.70087444982e-05\n",
      "epoch 0, batch 245 / 300: learn rate = 2.6864236149e-05\n",
      "epoch 0, batch 246 / 300: learn rate = 2.67205009813e-05\n",
      "epoch 0, batch 247 / 300: learn rate = 2.65775348583e-05\n",
      "epoch 0, batch 248 / 300: learn rate = 2.64353336653e-05\n",
      "epoch 0, batch 249 / 300: learn rate = 2.62938933095e-05\n",
      "epoch 0, batch 250 / 300: learn rate = 2.61532097202e-05\n",
      "epoch 0, batch 251 / 300: learn rate = 2.60132788484e-05\n",
      "epoch 0, batch 252 / 300: learn rate = 2.58740966667e-05\n",
      "epoch 0, batch 253 / 300: learn rate = 2.57356591692e-05\n",
      "epoch 0, batch 254 / 300: learn rate = 2.55979623717e-05\n",
      "epoch 0, batch 255 / 300: learn rate = 2.54610023109e-05\n",
      "epoch 0, batch 256 / 300: learn rate = 2.53247750452e-05\n",
      "epoch 0, batch 257 / 300: learn rate = 2.51892766536e-05\n",
      "epoch 0, batch 258 / 300: learn rate = 2.50545032365e-05\n",
      "epoch 0, batch 259 / 300: learn rate = 2.49204509149e-05\n",
      "epoch 0, batch 260 / 300: learn rate = 2.47871158306e-05\n",
      "\n",
      "Finished epoch 0, batch 260: accuracy = 0.99\n",
      "\n",
      "epoch 0, batch 261 / 300: learn rate = 2.46544941461e-05\n",
      "epoch 0, batch 262 / 300: learn rate = 2.45225820444e-05\n",
      "epoch 0, batch 263 / 300: learn rate = 2.43913757289e-05\n",
      "epoch 0, batch 264 / 300: learn rate = 2.42608714235e-05\n",
      "epoch 0, batch 265 / 300: learn rate = 2.41310653719e-05\n",
      "epoch 0, batch 266 / 300: learn rate = 2.40019538383e-05\n",
      "epoch 0, batch 267 / 300: learn rate = 2.38735331067e-05\n",
      "epoch 0, batch 268 / 300: learn rate = 2.3745799481e-05\n",
      "epoch 0, batch 269 / 300: learn rate = 2.36187492849e-05\n",
      "epoch 0, batch 270 / 300: learn rate = 2.34923788618e-05\n",
      "epoch 0, batch 271 / 300: learn rate = 2.33666845745e-05\n",
      "epoch 0, batch 272 / 300: learn rate = 2.32416628055e-05\n",
      "epoch 0, batch 273 / 300: learn rate = 2.31173099565e-05\n",
      "epoch 0, batch 274 / 300: learn rate = 2.29936224485e-05\n",
      "epoch 0, batch 275 / 300: learn rate = 2.28705967217e-05\n",
      "epoch 0, batch 276 / 300: learn rate = 2.27482292351e-05\n",
      "epoch 0, batch 277 / 300: learn rate = 2.2626516467e-05\n",
      "epoch 0, batch 278 / 300: learn rate = 2.25054549144e-05\n",
      "epoch 0, batch 279 / 300: learn rate = 2.23850410929e-05\n",
      "epoch 0, batch 280 / 300: learn rate = 2.22652715369e-05\n",
      "\n",
      "Finished epoch 0, batch 280: accuracy = 0.98\n",
      "\n",
      "epoch 0, batch 281 / 300: learn rate = 2.21461427993e-05\n",
      "epoch 0, batch 282 / 300: learn rate = 2.20276514515e-05\n",
      "epoch 0, batch 283 / 300: learn rate = 2.19097940831e-05\n",
      "epoch 0, batch 284 / 300: learn rate = 2.17925673022e-05\n",
      "epoch 0, batch 285 / 300: learn rate = 2.16759677347e-05\n",
      "epoch 0, batch 286 / 300: learn rate = 2.15599920248e-05\n",
      "epoch 0, batch 287 / 300: learn rate = 2.14446368347e-05\n",
      "epoch 0, batch 288 / 300: learn rate = 2.13298988442e-05\n",
      "epoch 0, batch 289 / 300: learn rate = 2.12157747511e-05\n",
      "epoch 0, batch 290 / 300: learn rate = 2.11022612707e-05\n",
      "epoch 0, batch 291 / 300: learn rate = 2.09893551361e-05\n",
      "epoch 0, batch 292 / 300: learn rate = 2.08770530977e-05\n",
      "epoch 0, batch 293 / 300: learn rate = 2.07653519232e-05\n",
      "epoch 0, batch 294 / 300: learn rate = 2.06542483979e-05\n",
      "epoch 0, batch 295 / 300: learn rate = 2.0543739324e-05\n",
      "epoch 0, batch 296 / 300: learn rate = 2.0433821521e-05\n",
      "epoch 0, batch 297 / 300: learn rate = 2.03244918253e-05\n",
      "epoch 0, batch 298 / 300: learn rate = 2.02157470903e-05\n",
      "epoch 0, batch 299 / 300: learn rate = 2.01075841862e-05\n",
      "\n",
      "END OF EPOCH 0: accuracy = 0.9635\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 200\n",
    "dropout_keep_rate = 0.5\n",
    "base_learn_rate = 0.0001\n",
    "test_every_n_batches = 20\n",
    "decay_factor = 5\n",
    "\n",
    "batches_per_epoch = train_im.shape[0] / batch_size\n",
    "for ep in xrange(num_epochs):\n",
    "    for i in xrange(batches_per_epoch):\n",
    "        this_learn_rate = base_learn_rate / (decay_factor ** (ep + (i / float(batches_per_epoch))))\n",
    "        print \"epoch {ep}, batch {ba} / {bpe}: learn rate = {lr}\".format(\n",
    "            ep=ep, ba=i, lr=this_learn_rate, bpe=batches_per_epoch)\n",
    "        start_offset = i * batch_size\n",
    "        stop_offset = start_offset + batch_size\n",
    "        train_step.run(feed_dict={\n",
    "            x: train_im[start_offset:stop_offset],\n",
    "            y_: train_labels_one_hot[start_offset:stop_offset],\n",
    "            keep_prob: dropout_keep_rate,\n",
    "            learn_rate: this_learn_rate})\n",
    "        if i % test_every_n_batches == 0:\n",
    "            ac = accuracy.eval(feed_dict={\n",
    "                x: test_im[:100],\n",
    "                y_: test_labels_one_hot[:100],\n",
    "                keep_prob: 1.0})\n",
    "            print \"\\nFinished epoch {ep}, batch {ba}: accuracy = {ac:.2f}\\n\".format(ep=ep, ba=i, ac=ac)\n",
    "    ac = accuracy.eval(feed_dict={\n",
    "        x: test_im,\n",
    "        y_: test_labels_one_hot,\n",
    "        keep_prob: 1.0})\n",
    "    print \"\\nEND OF EPOCH {ep}: accuracy = {ac:.4f}\\n\".format(ep=ep, ac=ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a random forest based on hidden layer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 in 1.82 seconds\n",
      "Batch 1 in 1.80 seconds\n",
      "Batch 2 in 2.56 seconds\n",
      "Batch 3 in 1.93 seconds\n",
      "Batch 4 in 1.79 seconds\n",
      "Batch 5 in 1.76 seconds\n",
      "Batch 6 in 1.76 seconds\n",
      "Batch 7 in 1.73 seconds\n",
      "Batch 8 in 1.74 seconds\n",
      "Batch 9 in 1.74 seconds\n",
      "Batch 10 in 1.76 seconds\n",
      "Batch 11 in 1.78 seconds\n",
      "Batch 12 in 1.75 seconds\n",
      "Batch 13 in 1.74 seconds\n",
      "Batch 14 in 1.78 seconds\n",
      "Batch 15 in 1.78 seconds\n",
      "Batch 16 in 1.77 seconds\n",
      "Batch 17 in 1.75 seconds\n",
      "Batch 18 in 1.76 seconds\n",
      "Batch 19 in 1.75 seconds\n",
      "Batch 20 in 1.77 seconds\n",
      "Batch 21 in 1.77 seconds\n",
      "Batch 22 in 1.75 seconds\n",
      "Batch 23 in 1.76 seconds\n",
      "Batch 24 in 1.77 seconds\n",
      "Batch 25 in 1.96 seconds\n",
      "Batch 26 in 2.26 seconds\n",
      "Batch 27 in 2.02 seconds\n",
      "Batch 28 in 1.77 seconds\n",
      "Batch 29 in 2.09 seconds\n",
      "Batch 30 in 1.77 seconds\n",
      "Batch 31 in 1.80 seconds\n",
      "Batch 32 in 1.81 seconds\n",
      "Batch 33 in 2.17 seconds\n",
      "Batch 34 in 2.16 seconds\n",
      "Batch 35 in 1.86 seconds\n",
      "Batch 36 in 1.81 seconds\n",
      "Batch 37 in 1.82 seconds\n",
      "Batch 38 in 1.81 seconds\n",
      "Batch 39 in 2.25 seconds\n",
      "Batch 40 in 1.82 seconds\n",
      "Batch 41 in 1.81 seconds\n",
      "Batch 42 in 1.96 seconds\n",
      "Batch 43 in 2.20 seconds\n",
      "Batch 44 in 1.92 seconds\n",
      "Batch 45 in 2.05 seconds\n",
      "Batch 46 in 2.30 seconds\n",
      "Batch 47 in 1.97 seconds\n",
      "Batch 48 in 2.31 seconds\n",
      "Batch 49 in 2.19 seconds\n",
      "Batch 50 in 1.93 seconds\n",
      "Batch 51 in 1.94 seconds\n",
      "Batch 52 in 1.82 seconds\n",
      "Batch 53 in 2.07 seconds\n",
      "Batch 54 in 2.42 seconds\n",
      "Batch 55 in 2.06 seconds\n",
      "Batch 56 in 2.21 seconds\n",
      "Batch 57 in 2.67 seconds\n",
      "Batch 58 in 1.87 seconds\n",
      "Batch 59 in 2.28 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "hidden_list = []\n",
    "for i in xrange(60):\n",
    "    mark = time.time()\n",
    "    hidden_list.append(\n",
    "        h_dropout.eval(feed_dict={\n",
    "            keep_prob: 1.0,\n",
    "            x: train_im[i * 1000:(i + 1) * 1000]}))\n",
    "    print \"Batch {b} in {t:.2f} seconds\".format(b=i, t=time.time() - mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_train = np.array(hidden_list).reshape(60000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 in 2.14 seconds\n",
      "Batch 1 in 2.08 seconds\n",
      "Batch 2 in 1.88 seconds\n",
      "Batch 3 in 2.15 seconds\n",
      "Batch 4 in 2.16 seconds\n",
      "Batch 5 in 1.83 seconds\n",
      "Batch 6 in 1.83 seconds\n",
      "Batch 7 in 1.80 seconds\n",
      "Batch 8 in 1.78 seconds\n",
      "Batch 9 in 1.86 seconds\n"
     ]
    }
   ],
   "source": [
    "hidden_list = []\n",
    "for i in xrange(10):\n",
    "    mark = time.time()\n",
    "    hidden_list.append(\n",
    "        h_dropout.eval(feed_dict={\n",
    "            keep_prob: 1.0,\n",
    "            x: test_im[i * 1000:(i + 1) * 1000]}))\n",
    "    print \"Batch {b} in {t:.2f} seconds\".format(b=i, t=time.time() - mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_test = np.array(hidden_list).reshape(10000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1)\n",
    "clf.fit(hidden_train, train_labels)\n",
    "model_labels = clf.predict(hidden_test)\n",
    "(model_labels == test_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
